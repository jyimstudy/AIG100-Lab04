{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 - Information Theory in Machine Learning\n",
    "\n",
    "Welcome to this week's lab on Information Theory! This week, we will dive into the fascinating world of Information Theory as applied to Machine Learning. Specifically, we will focus on two key concepts: Entropy and Information Gain. These principles are fundamental in understanding how decision trees make split decisions to organize data effectively.\n",
    "\n",
    "### Entropy\n",
    "- Entropy, in the context of information theory, measures the level of uncertainty or disorder within a set of data.\n",
    "- In machine learning, particularly in decision trees, entropy helps to determine how a dataset should be split. A high entropy means more disorder, indicating that our dataset is varied. Conversely, low entropy suggests more uniformity in the data.\n",
    "\n",
    "### Information Gain\n",
    "- Information Gain measures the reduction in entropy after the dataset is split on an attribute.\n",
    "- It is crucial in building decision trees as it helps to decide the order of attributes the tree will use for splitting the data. The attribute with the highest Information Gain is chosen as the splitting attribute at each node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Entropy and Information Gain in Decision Trees\n",
    "Decision Trees use these concepts to create branches. By choosing splits that maximize Information Gain (or equivalently minimize entropy), a decision tree can effectively categorize data, leading to better classification or regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load and Explore the Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Calculate Entropy\n",
    "To calculate the `entropy` we need to:\n",
    "- First, extract the target variable `y` from your dataset (like the 'target' column in the Iris dataset).\n",
    "- Then, call `calculate_entropy(y)` to get the entropy.\n",
    "\n",
    "This function calculates the entropy of a given target variable `y`. It works by first determining the unique classes in `y`, then computes the probability of each class, and uses this probability to calculate the entropy. This is a crucial step in understanding the disorder or uncertainty in the dataset, a fundamental concept in information theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(y):\n",
    "    class_labels = np.unique(y)\n",
    "    entropy = 0\n",
    "    for label in class_labels:\n",
    "        probability = len(y[y == label]) / len(y)\n",
    "        entropy -= probability * np.log2(probability)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the entropy for the target variable.  What is your observastion about the calculated Entropy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of the target variable: 1.584962500721156\n"
     ]
    }
   ],
   "source": [
    "entropy_target = calculate_entropy(df['target'])\n",
    "print(f\"Entropy of the target variable: {entropy_target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculated entropy of 1.584962500721156 for the Iris dataset's target variable indicates:\n",
    "\n",
    "The Iris dataset has three classes of iris (setosa, versicolor, and virginica).  An entropy close to the maximum possible entropy for a 3-class problem (which is log2(3) â‰ˆ 1.585) suggests that the classes are relatively balanced.  If one class were dominant, the entropy would be lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Calculate Information Gain\n",
    "There are three steps for calculating the Information Gain:\n",
    "1. Compute Overall Entropy: Use the entropy function from Step 3 on the entire target dataset.\n",
    "2. Calculate Weighted Entropy for Each Attribute: For each unique value in the attribute, partition the dataset and calculate its entropy. Then calculate the weighted sum of these entropies, where the weights are the proportions of instances in each partition.\n",
    "3. Compute Information Gain: Subtract the weighted entropy of the split from the original entropy.\n",
    "\n",
    "The attribute with the highest Information Gain is generally chosen for splitting, as it provides the most significant reduction in uncertainty. This step is critical in constructing an effective decision tree, as it directly influences the structure and depth of the tree.\n",
    "\n",
    "**Use the provided function to calculate the information gain for each of the features in the dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_information_gain(df, attribute, target_name):\n",
    "    total_entropy = calculate_entropy(df[target_name])\n",
    "    values, counts = np.unique(df[attribute], return_counts=True)\n",
    "    weighted_entropy = sum((counts[i] / sum(counts)) * calculate_entropy(df.where(df[attribute] == values[i]).dropna()[target_name]) for i in range(len(values)))\n",
    "    information_gain = total_entropy - weighted_entropy\n",
    "    return information_gain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss your findings here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain for sepal length (cm): 0.8769376208910578\n",
      "Information Gain for sepal width (cm): 0.5166428756804977\n",
      "Information Gain for petal length (cm): 1.4463165236458\n",
      "Information Gain for petal width (cm): 1.4358978386754417\n"
     ]
    }
   ],
   "source": [
    "feature_names = df.columns[:-1]  # Exclude the target column\n",
    "\n",
    "target_name = 'target'  # Define the target column name\n",
    "for feature in feature_names:\n",
    "    information_gain = calculate_information_gain(df, feature, target_name)\n",
    "    print(f\"Information Gain for {feature}: {information_gain}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings:\n",
    "\n",
    "+ **Petal Features are More Informative**: The petal length (1.446) and petal width (1.436) have significantly higher information gain than the sepal features. This indicates that the petal measurements are much more effective at distinguishing between the iris species than the sepal measurements.  \n",
    "\n",
    "+ **Sepal Features are Less Informative**: Sepal length (0.877) and sepal width (0.517) have considerably lower information gain.  Sepal measurements are less useful on their own for classifying the iris species. Sepal width, in particular, seems to be the least informative of the four features.\n",
    "\n",
    "+ **Predictive Power**:  A higher information gain suggests that a feature has more predictive power.  As a result, petal length or petal width shall be used as the first splitting feature for a decision tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Apply Entropy and Information Gain on a different dataset\n",
    "\n",
    "Your task is to choose a new dataset and implement what you learned in `Part 1` on this new dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set: \n",
    "Bohanec, M. (1988). Car Evaluation [Dataset]. UCI Machine Learning Repository. https://doi.org/10.24432/C5JP48.\n",
    "\n",
    "Car Evaluation Database was derived from a simple hierarchical decision model originally developed for the demonstration of DEX, M. Bohanec, V. Rajkovic: Expert system for decision making. Sistemica 1(1), pp. 145-157, 1990.). The model evaluates cars according to the following concept structure:\n",
    "\n",
    "CAR         car acceptability\n",
    " + buying    buying price\n",
    " + maint     price of the maintenance\n",
    " + doors     number of doors\n",
    " + persons   capacity in terms of persons to carry\n",
    " + lug_boot  the size of luggage boot\n",
    " + safety    estimated safety of the car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Implement Entropy and Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain for buying: 0.09644896916961376\n",
      "Information Gain for maint: 0.07370394692148574\n",
      "Information Gain for doors: 0.004485716626631886\n",
      "Information Gain for persons: 0.21966296333990798\n",
      "Information Gain for lug_boot: 0.030008141247605202\n",
      "Information Gain for safety: 0.26218435655426375\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "\n",
    "df = pd.read_csv('car.data', header=None)\n",
    "\n",
    "# Assign column names\n",
    "df.columns = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'target']\n",
    "\n",
    "feature_names = df.columns[:-1]  # Exclude the target column\n",
    "\n",
    "target_name = 'target'  # Define the target column name\n",
    "for feature in feature_names:\n",
    "    information_gain = calculate_information_gain(df, feature, target_name)\n",
    "    print(f\"Information Gain for {feature}: {information_gain}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Discuss your findings in detail\n",
    "Provide detailed explanation and discussion about your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information gain values provide valuable insights into which features of the Car Evaluation dataset are most influential in determining the car's acceptability.\n",
    "\n",
    "+ `safety` (0.262): This feature has the highest information gain.  It means that safety is the most crucial factor in predicting the car's evaluation.  Safety rating provides the most information about whether a car will be considered acceptable or not.  \n",
    "\n",
    "+ `persons` (0.220): The number of persons the car can accommodate is the second most important feature.  It means that car capacity is a significant factor in its overall evaluation.\n",
    "\n",
    "+ `buying` (0.096): The buying price also has a noticeable impact, though less than safety and capacity.  This is expected, as affordability is always a consideration.\n",
    "\n",
    "+ `maint` (0.074): Maintenance cost plays a role, but it's less influential than safety, capacity, or price.\n",
    "\n",
    "+ `lug_boot` (0.030): The size of the luggage boot has a relatively small information gain.  It's not as decisive as the other features.\n",
    "\n",
    "+ `doors` (0.004): The number of doors has the lowest information gain, close to zero.  It is the least important feature for predicting the car's evaluation in this dataset. The number of doors is not a strong factor between acceptable and unacceptable cars in this dataset.\n",
    "\n",
    "Information gain calculation is crucial for deciding how decision trees learn. It forms the foundation of feature selection and tree construction. The information gain values provide a clear picture of the relative importance of each feature in the Car Evaluation dataset and how a decision tree would likely use these features to make predictions.  \n",
    "\n",
    "Specifically, a decision tree will prioritize `safety` and `persons` features, which exhibit relatively greater information gain, leading to branches that effectively partition cars based on safety ratings and passenger capacity. After these initial splits, the data is divided into subsets. Within each subset, information gain is recalculated to determine the subsequent best feature for splitting, as the relative importance of the remaining features may vary.  Features are employed with careful consideration of potential overfitting. Techniques such as pruning or setting a maximum tree depth can be used to mitigate this risk. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "Submit your completed Jupyter Notebook file through the submission link in Blackboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
